#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£é“¾æ¥æ£€æŸ¥è„šæœ¬
æ£€æŸ¥æ–‡æ¡£ä¸­çš„æ‰€æœ‰é“¾æ¥ï¼Œç”Ÿæˆè¯¦ç»†çš„æ£€æŸ¥æŠ¥å‘Š
"""

import os
import re
import sys
import json
import time
import urllib.request
import urllib.error
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Set
from urllib.parse import urljoin, urlparse
import argparse

class LinkChecker:
    def __init__(self, base_dir: str = "docs", output_file: str = "link-check-report.json"):
        self.base_dir = Path(base_dir)
        self.output_file = output_file
        self.checked_urls: Set[str] = set()
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_files": 0,
                "total_links": 0,
                "valid_links": 0,
                "invalid_links": 0,
                "skipped_links": 0
            },
            "files": [],
            "invalid_links": [],
            "skipped_patterns": []
        }
        
        # å¿½ç•¥çš„é“¾æ¥æ¨¡å¼
        self.ignore_patterns = [
            r'^mailto:',
            r'^tel:',
            r'^#',
            r'^javascript:',
            r'^http://localhost',
            r'^https://localhost',
            r'^http://127\.0\.0\.1',
            r'^https://127\.0\.0\.1',
            r'\.(jpg|jpeg|png|gif|svg|ico|pdf)$'
        ]
        
        # HTTP è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; DocumentationLinkChecker/1.0)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

    def should_ignore_link(self, url: str) -> bool:
        """æ£€æŸ¥é“¾æ¥æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥"""
        for pattern in self.ignore_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True
        return False

    def extract_links_from_file(self, file_path: Path) -> List[Tuple[str, int]]:
        """ä» Markdown æ–‡ä»¶ä¸­æå–æ‰€æœ‰é“¾æ¥"""
        links = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŒ¹é… Markdown é“¾æ¥æ ¼å¼ [text](url)
            markdown_links = re.finditer(r'\[([^\]]*)\]\(([^)]+)\)', content)
            for match in markdown_links:
                url = match.group(2)
                line_num = content[:match.start()].count('\n') + 1
                links.append((url, line_num))
            
            # åŒ¹é… HTML é“¾æ¥æ ¼å¼ <a href="url">
            html_links = re.finditer(r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>', content, re.IGNORECASE)
            for match in html_links:
                url = match.group(1)
                line_num = content[:match.start()].count('\n') + 1
                links.append((url, line_num))
            
            # åŒ¹é…ç›´æ¥çš„ URL
            url_pattern = r'https?://[^\s<>"\'`\[\](){}]+'
            direct_urls = re.finditer(url_pattern, content)
            for match in direct_urls:
                url = match.group(0)
                line_num = content[:match.start()].count('\n') + 1
                # é¿å…é‡å¤æ·»åŠ å·²ç»åœ¨ Markdown æˆ– HTML é“¾æ¥ä¸­çš„ URL
                if not any(url in link[0] for link in links):
                    links.append((url, line_num))
                    
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return links

    def check_internal_link(self, url: str, current_file: Path) -> bool:
        """æ£€æŸ¥å†…éƒ¨é“¾æ¥ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰"""
        try:
            # å¤„ç†ç›¸å¯¹è·¯å¾„
            if url.startswith('./') or url.startswith('../') or not url.startswith(('http://', 'https://', '/')):
                # ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„è·¯å¾„
                target_path = (current_file.parent / url).resolve()
            elif url.startswith('/'):
                # ç›¸å¯¹äºæ–‡æ¡£æ ¹ç›®å½•çš„è·¯å¾„
                target_path = (self.base_dir / url.lstrip('/')).resolve()
            else:
                return True  # ä¸æ˜¯å†…éƒ¨é“¾æ¥
            
            # ç§»é™¤é”šç‚¹
            if '#' in str(target_path):
                target_path = Path(str(target_path).split('#')[0])
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if target_path.exists():
                return True
            
            # å¦‚æœæ˜¯ç›®å½•ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ index.md
            if target_path.is_dir():
                index_file = target_path / 'index.md'
                return index_file.exists()
            
            # å°è¯•æ·»åŠ  .md æ‰©å±•å
            if not target_path.suffix:
                md_file = target_path.with_suffix('.md')
                return md_file.exists()
                
            return False
            
        except Exception:
            return False

    def check_external_link(self, url: str) -> Tuple[bool, str]:
        """æ£€æŸ¥å¤–éƒ¨é“¾æ¥ï¼ˆHTTP/HTTPSï¼‰"""
        if url in self.checked_urls:
            return True, "å·²æ£€æŸ¥è¿‡"
        
        try:
            req = urllib.request.Request(url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=30) as response:
                status_code = response.getcode()
                self.checked_urls.add(url)
                if status_code in [200, 301, 302, 403]:  # 403 å¯èƒ½æ˜¯é˜²çˆ¬è™«ï¼Œä½†é“¾æ¥å¯èƒ½æœ‰æ•ˆ
                    return True, f"HTTP {status_code}"
                else:
                    return False, f"HTTP {status_code}"
                    
        except urllib.error.HTTPError as e:
            self.checked_urls.add(url)
            if e.code in [403, 429]:  # å¯èƒ½æ˜¯é˜²çˆ¬è™«
                return True, f"HTTP {e.code} (å¯èƒ½è¢«é˜²çˆ¬è™«ä¿æŠ¤)"
            return False, f"HTTP {e.code}"
            
        except urllib.error.URLError as e:
            self.checked_urls.add(url)
            return False, f"URLé”™è¯¯: {e.reason}"
            
        except Exception as e:
            self.checked_urls.add(url)
            return False, f"æ£€æŸ¥å¤±è´¥: {str(e)}"

    def check_link(self, url: str, current_file: Path) -> Tuple[bool, str]:
        """æ£€æŸ¥å•ä¸ªé“¾æ¥"""
        # æ¸…ç† URL
        url = url.strip()
        
        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹è¿›è¡Œæ£€æŸ¥ï¼ˆä½†ä¿ç•™åŸå§‹ URL ç”¨äºæŠ¥å‘Šï¼‰
        clean_url = url.split('?')[0].split('#')[0]
        
        if url.startswith(('http://', 'https://')):
            return self.check_external_link(url)
        else:
            is_valid = self.check_internal_link(clean_url, current_file)
            return is_valid, "å†…éƒ¨é“¾æ¥" if is_valid else "æ–‡ä»¶ä¸å­˜åœ¨"

    def check_file(self, file_path: Path) -> Dict:
        """æ£€æŸ¥å•ä¸ªæ–‡ä»¶ä¸­çš„æ‰€æœ‰é“¾æ¥"""
        print(f"ğŸ“„ æ£€æŸ¥æ–‡ä»¶: {file_path}")
        
        file_result = {
            "file": str(file_path.relative_to(Path.cwd())),
            "links": [],
            "summary": {
                "total": 0,
                "valid": 0,
                "invalid": 0,
                "skipped": 0
            }
        }
        
        links = self.extract_links_from_file(file_path)
        file_result["summary"]["total"] = len(links)
        
        for url, line_num in links:
            if self.should_ignore_link(url):
                file_result["summary"]["skipped"] += 1
                file_result["links"].append({
                    "url": url,
                    "line": line_num,
                    "status": "skipped",
                    "message": "åŒ¹é…å¿½ç•¥æ¨¡å¼"
                })
                continue
            
            is_valid, message = self.check_link(url, file_path)
            
            link_result = {
                "url": url,
                "line": line_num,
                "status": "valid" if is_valid else "invalid",
                "message": message
            }
            
            file_result["links"].append(link_result)
            
            if is_valid:
                file_result["summary"]["valid"] += 1
                print(f"  âœ… {url}")
            else:
                file_result["summary"]["invalid"] += 1
                print(f"  âŒ {url} - {message}")
                
                # æ·»åŠ åˆ°å…¨å±€æ— æ•ˆé“¾æ¥åˆ—è¡¨
                self.results["invalid_links"].append({
                    "file": str(file_path.relative_to(Path.cwd())),
                    "url": url,
                    "line": line_num,
                    "message": message
                })
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        return file_result

    def run(self) -> bool:
        """è¿è¡Œé“¾æ¥æ£€æŸ¥"""
        print(f"ğŸ” å¼€å§‹æ£€æŸ¥æ–‡æ¡£é“¾æ¥...")
        print(f"ğŸ“ æ£€æŸ¥ç›®å½•: {self.base_dir}")
        
        if not self.base_dir.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {self.base_dir}")
            return False
        
        # æŸ¥æ‰¾æ‰€æœ‰ Markdown æ–‡ä»¶
        md_files = list(self.base_dir.rglob("*.md"))
        
        # ä¹Ÿæ£€æŸ¥æ ¹ç›®å½•çš„ README æ–‡ä»¶
        for readme in ["README.md", "README-EN.md"]:
            readme_path = Path(readme)
            if readme_path.exists():
                md_files.append(readme_path)
        
        if not md_files:
            print("âš ï¸  æœªæ‰¾åˆ° Markdown æ–‡ä»¶")
            return True
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")
        
        self.results["summary"]["total_files"] = len(md_files)
        
        # æ£€æŸ¥æ¯ä¸ªæ–‡ä»¶
        for file_path in md_files:
            file_result = self.check_file(file_path)
            self.results["files"].append(file_result)
            
            # æ›´æ–°æ€»è®¡
            self.results["summary"]["total_links"] += file_result["summary"]["total"]
            self.results["summary"]["valid_links"] += file_result["summary"]["valid"]
            self.results["summary"]["invalid_links"] += file_result["summary"]["invalid"]
            self.results["summary"]["skipped_links"] += file_result["summary"]["skipped"]
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
        
        # è¾“å‡ºæ€»ç»“
        summary = self.results["summary"]
        print(f"\nğŸ“Š æ£€æŸ¥å®Œæˆ:")
        print(f"  ğŸ“ æ–‡ä»¶æ•°é‡: {summary['total_files']}")
        print(f"  ğŸ”— é“¾æ¥æ€»æ•°: {summary['total_links']}")
        print(f"  âœ… æœ‰æ•ˆé“¾æ¥: {summary['valid_links']}")
        print(f"  âŒ æ— æ•ˆé“¾æ¥: {summary['invalid_links']}")
        print(f"  â­ï¸  è·³è¿‡é“¾æ¥: {summary['skipped_links']}")
        
        if summary['invalid_links'] > 0:
            print(f"\nâŒ å‘ç° {summary['invalid_links']} ä¸ªæ— æ•ˆé“¾æ¥:")
            for invalid_link in self.results["invalid_links"]:
                print(f"  - {invalid_link['file']}:{invalid_link['line']} - {invalid_link['url']}")
                print(f"    é”™è¯¯: {invalid_link['message']}")
        
        return summary['invalid_links'] == 0

    def generate_report(self):
        """ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š"""
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ: {self.output_file}")
        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

def main():
    parser = argparse.ArgumentParser(description='æ£€æŸ¥æ–‡æ¡£ä¸­çš„é“¾æ¥æœ‰æ•ˆæ€§')
    parser.add_argument('--dir', default='docs', help='è¦æ£€æŸ¥çš„æ–‡æ¡£ç›®å½• (é»˜è®¤: docs)')
    parser.add_argument('--output', default='link-check-report.json', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶ (é»˜è®¤: link-check-report.json)')
    parser.add_argument('--fail-on-error', action='store_true', help='å‘ç°æ— æ•ˆé“¾æ¥æ—¶é€€å‡ºç ä¸º1')
    
    args = parser.parse_args()
    
    checker = LinkChecker(args.dir, args.output)
    success = checker.run()
    
    if args.fail_on_error and not success:
        sys.exit(1)

if __name__ == "__main__":
    main()